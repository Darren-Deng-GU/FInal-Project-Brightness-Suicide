{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suiside："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to /Users/cassiezhang/Desktop/DSPP/Year 1/ds/finial_project/HDPulse_data_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with the correct file path\n",
    "\n",
    "csv_file_path = '/Users/cassiezhang/Desktop/DSPP/Year 1/ds/finial_project/HDPulse_data_export.csv'\n",
    "# Update with your file path\n",
    "\n",
    "# Try reading with a semicolon delimiter or adjust if needed\n",
    "data_cleaned = pd.read_csv(csv_file_path, delimiter=',', skiprows=3, on_bad_lines='skip')\n",
    "\n",
    "# Remove rows from index 70 to 93 (because Python uses 0-based indexing, 71st row is at index 70)\n",
    "if data_cleaned.shape[0] > 70:\n",
    "    data_cleaned = data_cleaned.drop(index=range(64, 80))  # delete 70 to 93 line \n",
    "else:\n",
    "    print(\"if the number of data rows is less than expected, the deletion step is skipped\")\n",
    "\n",
    "# Keep only the necessary columns: 'County', 'Average Annual Count', 'Age-Adjusted Death Rate(†) - deaths per 100,000'\n",
    "data_filtered = data_cleaned[['County', 'Average Annual Count', 'Age-Adjusted Death Rate(†) - deaths per 100,000']]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_csv_path = '/Users/cassiezhang/Desktop/DSPP/Year 1/ds/finial_project/HDPulse_data_filtered.csv'  # Update with your desired output path\n",
    "data_filtered.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {filtered_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " forest coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理后的数据集已保存到 cleaned_treecover_loss_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the script directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Use relative paths to read files\n",
    "file_name = \"treecover_loss_by_region__ha.csv\"  # Make sure the file is in the same directory as the script\n",
    "file_path = os.path.join(current_dir, file_name)\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# keep column we need\n",
    "columns_to_keep = ['adm2', 'umd_tree_cover_loss__year', 'umd_tree_cover_loss__ha', 'gfw_gross_emissions_co2e_all_gases__Mg']\n",
    "cleaned_df = df[columns_to_keep]\n",
    "\n",
    "# clean dataset\n",
    "output_file_name = \"cleaned_treecover_loss_data.csv\"\n",
    "output_path = os.path.join(current_dir, output_file_name)\n",
    "cleaned_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"清理后的数据集已保存到 {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 年数据已处理并保存到 tree_cover_loss_2019_with_county_names_2019.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zh/q1nx6lrn0m31z31n4vtjk_fw0000gn/T/ipykernel_66144/4033669089.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.rename(columns={\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read file\n",
    "tree_cover_df = pd.read_csv(\"cleaned_treecover_loss_data.csv\")\n",
    "adm2_metadata_df = pd.read_csv(\"adm2_metadata.csv\")\n",
    "\n",
    "#Ensure that column names are consistent for easy merging\n",
    "adm2_metadata_df.rename(columns={\"adm2__id\": \"adm2\", \"name\": \"county_name\"}, inplace=True)\n",
    "\n",
    "# Make sure the data types are consistent\n",
    "tree_cover_df[\"adm2\"] = tree_cover_df[\"adm2\"].astype(str)\n",
    "adm2_metadata_df[\"adm2\"] = adm2_metadata_df[\"adm2\"].astype(str)\n",
    "\n",
    "# Merge the two data sets and match using adm2\n",
    "merged_df = pd.merge(tree_cover_df, adm2_metadata_df, on=\"adm2\", how=\"left\")\n",
    "\n",
    "# use county name to replace adm2\n",
    "merged_df[\"adm2\"] = merged_df[\"county_name\"]\n",
    "\n",
    "# only keep 2019 dataset\n",
    "filtered_df = merged_df[merged_df[\"umd_tree_cover_loss__year\"] == 2019]\n",
    "\n",
    "# change column name\n",
    "filtered_df.rename(columns={\n",
    "    \"umd_tree_cover_loss__ha\": \"tree_cover_loss_2019\",\n",
    "    \"gfw_gross_emissions_co2e_all_gases__Mg\": \"gross_emissions_co2e_all_gases_Mg_2019\"\n",
    "}, inplace=True)\n",
    "\n",
    "# keep relative column\n",
    "final_df = filtered_df[[\"adm2\", \"umd_tree_cover_loss__year\", \"tree_cover_loss_2019\", \"gross_emissions_co2e_all_gases_Mg_2019\"]]\n",
    "\n",
    "# save data to CSV file \n",
    "output_path = \"tree_cover_loss_2019_with_county_names_2019.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"2019 年数据已处理并保存到 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始列名: Index(['GEO_ID', 'NAME', 'AREALAND', 'AREAWATR', 'AREALAND_SQMI',\n",
      "       'AREAWATR_SQMI', 'INTPTLAT', 'INTPTLON'],\n",
      "      dtype='object')\n",
      "清理后的数据已保存到 GEOINFO2023_Cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read orginal data\n",
    "input_file = \"GEOINFO2023.GEOINFO-Data.csv\"  \n",
    "output_file = \"GEOINFO2023_Cleaned.csv\"  \n",
    "\n",
    "# check CSV \n",
    "geo_data = pd.read_csv(input_file)\n",
    "\n",
    "# check column name\n",
    "print(\"原始列名:\", geo_data.columns)\n",
    "\n",
    "# make sure 'NAME' exist\n",
    "if \"NAME\" in geo_data.columns:\n",
    "    # delete 'NAME', New York\"\n",
    "    geo_data[\"NAME\"] = geo_data[\"NAME\"].str.replace(\", New York\", \"\", regex=False).str.strip()\n",
    "\n",
    "    # save clean data\n",
    "    geo_data.to_csv(output_file, index=False)\n",
    "    print(f\"清理后的数据已保存到 {output_file}\")\n",
    "else:\n",
    "    print(\"错误: 数据中未找到 'NAME' 列，请检查文件格式。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到 merged_data_with_geo_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "merged_data = pd.read_csv(\"merged_data_final.csv\")\n",
    "geo_data = pd.read_csv(\"GEOINFO2023_Cleaned.csv\")\n",
    "\n",
    "# check column name consistent\n",
    "geo_data.rename(columns={\n",
    "    \"NAME\": \"County_Name\",  # Match the County Name column of merged data\n",
    "    \"INTPTLAT\": \"latitude\",  \n",
    "    \"INTPTLON\": \"longitude\"  \n",
    "}, inplace=True)\n",
    "\n",
    "# Delete the \"County\" suffix to make sure the names are consistent\n",
    "geo_data[\"County_Name\"] = geo_data[\"County_Name\"].str.replace(\" County\", \"\", regex=False)\n",
    "\n",
    "# Ensure the data type is consistent\n",
    "merged_data[\"County_Name\"] = merged_data[\"County_Name\"].astype(str)\n",
    "geo_data[\"County_Name\"] = geo_data[\"County_Name\"].astype(str)\n",
    "\n",
    "# combine data\n",
    "merged_with_geo = pd.merge(\n",
    "    merged_data, \n",
    "    geo_data[[\"County_Name\", \"latitude\", \"longitude\"]], \n",
    "    on=\"County_Name\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check if there is still missing latitude and longitude information\n",
    "missing_geo = merged_with_geo[merged_with_geo[\"latitude\"].isnull()]\n",
    "if not missing_geo.empty:\n",
    "    print(\"The following counties still lack latitude and longitude information\")\n",
    "    print(missing_geo[\"County_Name\"].unique())\n",
    "\n",
    "# save result to csv\n",
    "output_path = \"merged_data_with_geo_updated.csv\"\n",
    "merged_with_geo.to_csv(output_path, index=False)\n",
    "print(f\"数据已保存到 {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
